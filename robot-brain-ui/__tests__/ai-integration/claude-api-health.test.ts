/**
 * Claude API Integration Health Tests
 * Component 2: Production Health Verification Test Suite
 * 
 * TDD RED-GREEN-REFACTOR Pattern Implementation
 * Tests Claude API integration health with specific focus on:
 * - API response quality and latency
 * - Token usage optimization (100 tokens max)
 * - Error handling and fallback scenarios
 * - Conversation flow validation
 */

import { NextRequest } from 'next/server';
import { POST } from '@/app/api/chat/route';
import { POST as ValidateAnthropic } from '@/app/api/deployment/validate-anthropic/route';

// Mock Anthropic SDK with detailed response tracking
jest.mock('@anthropic-ai/sdk', () => {
  const mockCreate = jest.fn();
  const MockAnthropic = jest.fn().mockImplementation(() => ({
    messages: {
      create: mockCreate,
    },
  }));
  MockAnthropic.mockCreate = mockCreate;
  return MockAnthropic;
});

// Mock database for conversation tracking
jest.mock('@neondatabase/serverless', () => {
  const mockSql = jest.fn().mockResolvedValue({ rows: [] });
  return {
    neon: jest.fn(() => mockSql),
    mockSql,
  };
});

// Mock validation module
jest.mock('@/lib/validation', () => ({
  schemas: {
    chatRequest: {
      safeParse: jest.fn().mockReturnValue({
        success: true,
        data: { message: 'Test message', personality: 'robot-friend', sessionId: 'test-session' }
      })
    }
  },
  sanitizeInput: jest.fn((input) => input),
  validateSessionId: jest.fn((id) => id || 'default'),
  checkRateLimit: jest.fn(() => true),
  getClientIP: jest.fn(() => '127.0.0.1'),
}));

// Mock response cache
jest.mock('@/lib/response-cache', () => ({
  responseCache: {
    get: jest.fn(),
    set: jest.fn(),
    generateKey: jest.fn(() => 'cache-key'),
    has: jest.fn().mockReturnValue(false),
  },
  logCachePerformance: jest.fn(),
}));

// Mock robot config
jest.mock('@/lib/robot-config', () => ({
  ROBOT_PERSONALITIES: {
    'robot-friend': {
      id: 'robot-friend',
      name: 'Robot Friend',
      emoji: 'ðŸ˜Š',
      systemPrompt: 'You are a helpful, concise assistant. Keep responses brief and clear.',
      voice_id: '21m00Tcm4TlvDq8ikWAM',
    },
  },
}));

const PRODUCTION_URL = 'https://robot-brain-owaxqerjd-scientia-capital.vercel.app';
const CLAUDE_MODEL = 'claude-3-haiku-20240307';
const MAX_TOKENS = 100;
const TEMPERATURE = 0.3;
const LATENCY_TARGET = 2000; // 2 second target for Claude API

describe('Claude API Integration Health Tests', () => {
  const MockAnthropic = jest.mocked(require('@anthropic-ai/sdk'));
  const mockCreate = MockAnthropic.mockCreate;
  const mockValidation = jest.mocked(require('@/lib/validation'));
  const { responseCache } = jest.mocked(require('@/lib/response-cache'));

  beforeEach(() => {
    jest.clearAllMocks();
    
    // Default mock for successful Claude response
    mockCreate.mockResolvedValue({
      id: 'msg_test_123',
      model: CLAUDE_MODEL,
      role: 'assistant',
      content: [{ type: 'text', text: 'Test response from Claude' }],
      usage: { input_tokens: 25, output_tokens: 8 },
    });

    // Reset validation mock
    mockValidation.schemas.chatRequest.safeParse.mockReturnValue({
      success: true,
      data: { message: 'Test message', personality: 'robot-friend', sessionId: 'test-session' }
    });
    
    // Reset cache mock
    responseCache.get.mockReturnValue(null);
  });

  describe('RED Phase - Claude API Configuration Tests', () => {
    test('should use correct Claude model (claude-3-haiku-20240307)', async () => {
      const request = new Request('http://localhost:3000/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: 'Test Claude model configuration',
          personality: 'robot-friend',
        }),
      }) as unknown as NextRequest;

      await POST(request);

      expect(mockCreate).toHaveBeenCalledWith(
        expect.objectContaining({
          model: CLAUDE_MODEL,
        })
      );
    });

    test('should enforce 100 token limit for responses', async () => {
      const request = new Request('http://localhost:3000/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: 'Explain quantum computing in detail',
          personality: 'robot-friend',
        }),
      }) as unknown as NextRequest;

      await POST(request);

      expect(mockCreate).toHaveBeenCalledWith(
        expect.objectContaining({
          max_tokens: MAX_TOKENS,
        })
      );
    });

    test('should use temperature 0.3 for consistent responses', async () => {
      const request = new Request('http://localhost:3000/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: 'What is 2+2?',
          personality: 'robot-friend',
        }),
      }) as unknown as NextRequest;

      await POST(request);

      expect(mockCreate).toHaveBeenCalledWith(
        expect.objectContaining({
          temperature: TEMPERATURE,
        })
      );
    });
  });

  describe('GREEN Phase - Response Quality Validation', () => {
    test('should validate Claude response structure', async () => {
      mockCreate.mockResolvedValueOnce({
        id: 'msg_quality_test',
        model: CLAUDE_MODEL,
        role: 'assistant',
        content: [{ type: 'text', text: 'High quality response with proper structure' }],
        usage: { input_tokens: 30, output_tokens: 10 },
      });

      const request = new Request('http://localhost:3000/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: 'Test response quality',
          personality: 'robot-friend',
        }),
      }) as unknown as NextRequest;

      const response = await POST(request);
      const data = await response.json();

      expect(response.status).toBe(200);
      expect(data).toHaveProperty('message');
      expect(data).toHaveProperty('responseTime');
      expect(typeof data.message).toBe('string');
      expect(data.message.length).toBeGreaterThan(0);
      expect(data.message.length).toBeLessThanOrEqual(500); // Reasonable length for 100 tokens
    });

    test('should measure and report Claude API latency', async () => {
      const startTime = Date.now();
      
      // Simulate realistic API delay
      mockCreate.mockImplementation(() => {
        return new Promise((resolve) => {
          setTimeout(() => {
            resolve({
              id: 'msg_latency_test',
              model: CLAUDE_MODEL,
              role: 'assistant',
              content: [{ type: 'text', text: 'Response with measured latency' }],
              usage: { input_tokens: 20, output_tokens: 5 },
            });
          }, 500); // 500ms simulated latency
        });
      });

      const request = new Request('http://localhost:3000/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: 'Test latency measurement',
          personality: 'robot-friend',
        }),
      }) as unknown as NextRequest;

      const response = await POST(request);
      const data = await response.json();
      const endTime = Date.now();

      expect(response.status).toBe(200);
      expect(data.responseTime).toBeGreaterThanOrEqual(500);
      expect(data.responseTime).toBeLessThan(LATENCY_TARGET);
      expect(endTime - startTime).toBeGreaterThanOrEqual(500);
    });

    test('should validate token usage stays within limits', async () => {
      mockCreate.mockResolvedValueOnce({
        id: 'msg_token_test',
        model: CLAUDE_MODEL,
        role: 'assistant',
        content: [{ type: 'text', text: 'Brief response' }],
        usage: { input_tokens: 45, output_tokens: 95 }, // Total 140, output under 100
      });

      const request = new Request('http://localhost:3000/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: 'Generate a very long response about everything',
          personality: 'robot-friend',
        }),
      }) as unknown as NextRequest;

      const response = await POST(request);
      const data = await response.json();

      expect(response.status).toBe(200);
      expect(mockCreate).toHaveBeenCalledWith(
        expect.objectContaining({
          max_tokens: MAX_TOKENS,
        })
      );
    });
  });

  describe('REFACTOR Phase - Error Handling and Recovery', () => {
    test('should handle Claude API rate limiting gracefully', async () => {
      mockCreate.mockRejectedValueOnce({
        status: 429,
        message: 'Rate limit exceeded',
        headers: {
          'retry-after': '60',
        },
      });

      const request = new Request('http://localhost:3000/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: 'Test rate limit handling',
          personality: 'robot-friend',
        }),
      }) as unknown as NextRequest;

      const response = await POST(request);
      const data = await response.json();

      expect(response.status).toBe(500);
      expect(data).toHaveProperty('error');
      expect(data.error).toContain('Failed to process chat message');
    });

    test('should handle Claude API timeout scenarios', async () => {
      mockCreate.mockImplementation(() => {
        return new Promise((_, reject) => {
          setTimeout(() => {
            reject(new Error('Request timeout'));
          }, 5000);
        });
      });

      const request = new Request('http://localhost:3000/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: 'Test timeout handling',
          personality: 'robot-friend',
        }),
      }) as unknown as NextRequest;

      const responsePromise = POST(request);
      
      // Fast-forward time to simulate timeout
      jest.advanceTimersByTime(5000);
      
      const response = await responsePromise;
      const data = await response.json();

      expect(response.status).toBe(500);
      expect(data).toHaveProperty('error');
    });

    test('should implement retry logic for transient failures', async () => {
      let attemptCount = 0;
      
      mockCreate.mockImplementation(() => {
        attemptCount++;
        if (attemptCount === 1) {
          return Promise.reject(new Error('Temporary network error'));
        }
        return Promise.resolve({
          id: 'msg_retry_success',
          model: CLAUDE_MODEL,
          role: 'assistant',
          content: [{ type: 'text', text: 'Success after retry' }],
          usage: { input_tokens: 20, output_tokens: 5 },
        });
      });

      // Note: This test assumes retry logic is implemented in the API route
      // For now it will fail as per TDD RED phase
      const request = new Request('http://localhost:3000/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: 'Test retry logic',
          personality: 'robot-friend',
        }),
      }) as unknown as NextRequest;

      const response = await POST(request);
      
      // In RED phase, this will fail without retry logic
      expect(response.status).toBe(500);
    });

    test('should use cache for repeated identical requests', async () => {
      responseCache.get.mockReturnValueOnce('Cached response for efficiency');

      const request = new Request('http://localhost:3000/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: 'What is the weather?',
          personality: 'robot-friend',
        }),
      }) as unknown as NextRequest;

      const response = await POST(request);
      const data = await response.json();

      expect(response.status).toBe(200);
      expect(data.message).toBe('Cached response for efficiency');
      expect(data.cached).toBe(true);
      expect(mockCreate).not.toHaveBeenCalled();
    });
  });

  describe('Conversation Flow Validation', () => {
    test('should maintain conversation context across multiple messages', async () => {
      const sessionId = 'test-conversation-flow';
      
      // First message
      mockValidation.schemas.chatRequest.safeParse.mockReturnValueOnce({
        success: true,
        data: { 
          message: 'My favorite color is blue', 
          personality: 'robot-friend', 
          sessionId 
        }
      });

      mockCreate.mockResolvedValueOnce({
        id: 'msg_1',
        model: CLAUDE_MODEL,
        role: 'assistant',
        content: [{ type: 'text', text: 'I understand your favorite color is blue!' }],
        usage: { input_tokens: 10, output_tokens: 8 },
      });

      const request1 = new Request('http://localhost:3000/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: 'My favorite color is blue',
          personality: 'robot-friend',
          sessionId,
        }),
      }) as unknown as NextRequest;

      await POST(request1);

      // Second message referencing context
      mockValidation.schemas.chatRequest.safeParse.mockReturnValueOnce({
        success: true,
        data: { 
          message: 'What is my favorite color?', 
          personality: 'robot-friend', 
          sessionId 
        }
      });

      mockCreate.mockResolvedValueOnce({
        id: 'msg_2',
        model: CLAUDE_MODEL,
        role: 'assistant',
        content: [{ type: 'text', text: 'Your favorite color is blue!' }],
        usage: { input_tokens: 25, output_tokens: 6 },
      });

      const request2 = new Request('http://localhost:3000/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: 'What is my favorite color?',
          personality: 'robot-friend',
          sessionId,
        }),
      }) as unknown as NextRequest;

      const response2 = await POST(request2);
      const data2 = await response2.json();

      expect(response2.status).toBe(200);
      expect(mockCreate).toHaveBeenCalledTimes(2);
      
      // Verify context was included in second call
      const secondCall = mockCreate.mock.calls[1][0];
      expect(secondCall.messages).toHaveLength(3);
      expect(secondCall.messages[0].content).toContain('blue');
    });

    test('should handle conversation history limits (10 messages)', async () => {
      const sessionId = 'test-history-limit';
      
      // Simulate 15 messages
      for (let i = 0; i < 15; i++) {
        mockValidation.schemas.chatRequest.safeParse.mockReturnValueOnce({
          success: true,
          data: { 
            message: `Message ${i}`, 
            personality: 'robot-friend', 
            sessionId 
          }
        });

        mockCreate.mockResolvedValueOnce({
          id: `msg_${i}`,
          model: CLAUDE_MODEL,
          role: 'assistant',
          content: [{ type: 'text', text: `Response ${i}` }],
          usage: { input_tokens: 10, output_tokens: 5 },
        });

        const request = new Request('http://localhost:3000/api/chat', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            message: `Message ${i}`,
            personality: 'robot-friend',
            sessionId,
          }),
        }) as unknown as NextRequest;

        await POST(request);
      }

      expect(mockCreate).toHaveBeenCalledTimes(15);
      
      // Verify that history is being maintained (last call should have limited history)
      const lastCall = mockCreate.mock.calls[14][0];
      expect(lastCall.messages.length).toBeLessThanOrEqual(21); // 10 pairs + current
    });
  });

  describe('Production Health Validation', () => {
    test('should validate Anthropic API key configuration', async () => {
      const request = new Request('http://localhost:3000/api/deployment/validate-anthropic', {
        method: 'GET',
      }) as unknown as NextRequest;

      // Mock environment variable
      process.env.ANTHROPIC_API_KEY = 'test-api-key';

      const response = await ValidateAnthropic(request);
      const data = await response.json();

      expect(response.status).toBe(200);
      expect(data).toHaveProperty('status', 'healthy');
      expect(data).toHaveProperty('configured', true);
    });

    test('should measure end-to-end Claude integration performance', async () => {
      const performanceMetrics = {
        requestStart: Date.now(),
        apiCallStart: 0,
        apiCallEnd: 0,
        responseEnd: 0,
      };

      mockCreate.mockImplementation(() => {
        performanceMetrics.apiCallStart = Date.now();
        return new Promise((resolve) => {
          setTimeout(() => {
            performanceMetrics.apiCallEnd = Date.now();
            resolve({
              id: 'msg_perf_test',
              model: CLAUDE_MODEL,
              role: 'assistant',
              content: [{ type: 'text', text: 'Performance test response' }],
              usage: { input_tokens: 15, output_tokens: 4 },
            });
          }, 300);
        });
      });

      const request = new Request('http://localhost:3000/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: 'Performance test',
          personality: 'robot-friend',
        }),
      }) as unknown as NextRequest;

      const response = await POST(request);
      const data = await response.json();
      performanceMetrics.responseEnd = Date.now();

      expect(response.status).toBe(200);
      expect(data.responseTime).toBeGreaterThanOrEqual(300);
      expect(data.responseTime).toBeLessThan(LATENCY_TARGET);
      
      const totalTime = performanceMetrics.responseEnd - performanceMetrics.requestStart;
      expect(totalTime).toBeLessThan(LATENCY_TARGET + 500); // Allow 500ms overhead
    });

    test('should validate Claude response content quality', async () => {
      mockCreate.mockResolvedValueOnce({
        id: 'msg_quality_check',
        model: CLAUDE_MODEL,
        role: 'assistant',
        content: [{ 
          type: 'text', 
          text: 'This is a high-quality, coherent response that addresses the user query directly.' 
        }],
        usage: { input_tokens: 20, output_tokens: 15 },
      });

      const request = new Request('http://localhost:3000/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          message: 'Explain photosynthesis briefly',
          personality: 'robot-friend',
        }),
      }) as unknown as NextRequest;

      const response = await POST(request);
      const data = await response.json();

      expect(response.status).toBe(200);
      expect(data.message).toBeTruthy();
      expect(data.message.length).toBeGreaterThan(10);
      expect(data.message).not.toContain('undefined');
      expect(data.message).not.toContain('null');
      expect(data.message).not.toContain('error');
    });
  });
});